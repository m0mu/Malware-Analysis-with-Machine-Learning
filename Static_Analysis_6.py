
# coding: utf-8

# In[ ]:


# For Decision Tree + Forests


# In[1]:


import pefile
from collections import OrderedDict
import glob, sys, os
import matplotlib as mp
import matplotlib.pyplot as plt
import pandas as pd

list_of_benign_dict = []
list_of_malicious_dict = []
attr_dict = {}
list_of_files = ""

def DOS_HEADER():
    global attr_dict
    try:
        attr_dict["e_cblp"] = pe.DOS_HEADER.e_cblp
        attr_dict["e_oemid"] = pe.DOS_HEADER.e_oemid
        attr_dict["e_oeminfo"] = pe.DOS_HEADER.e_oeminfo       
        attr_dict["e_lfanew"] = pe.DOS_HEADER.e_lfanew
    
    except Exception as e:
        print("")

def FILE_HEADER():
    global attr_dict
    try:
        attr_dict["Machine"] = pe.FILE_HEADER.Machine
        attr_dict["NumberOfSections"] = pe.FILE_HEADER.NumberOfSections       
        attr_dict["Characteristics"] = pe.FILE_HEADER.Characteristics
        
    except Exception as e:
        print("")

def OPTIONAL_HEADER():
    global attr_dict
    try:
        
        attr_dict["MinorLinkerVersion"] = pe.OPTIONAL_HEADER.MinorLinkerVersion
        attr_dict["SizeOfCode"] = pe.OPTIONAL_HEADER.SizeOfCode
        attr_dict["SizeOfInitializedData"] = pe.OPTIONAL_HEADER.SizeOfInitializedData
        attr_dict["SizeOfUninitializedData"] = pe.OPTIONAL_HEADER.SizeOfUninitializedData
        attr_dict["MinorImageVersion"] = pe.OPTIONAL_HEADER.MinorImageVersion
        attr_dict["MajorLinkerVersion"] = pe.OPTIONAL_HEADER.MajorLinkerVersion
        attr_dict["BaseOfCode"] = pe.OPTIONAL_HEADER.BaseOfCode
        attr_dict["BaseOfData"] = pe.OPTIONAL_HEADER.BaseOfData
        attr_dict["CheckSum"] = pe.OPTIONAL_HEADER.CheckSum
        attr_dict["SizeOfHeapReserve"] = pe.OPTIONAL_HEADER.SizeOfHeapReserve
        attr_dict["SizeOfStackCommit"] = pe.OPTIONAL_HEADER.SizeOfStackCommit
        attr_dict["SizeOfStackReserve"] = pe.OPTIONAL_HEADER.SizeOfStackReserve
        attr_dict["DllCharacteristics"] = pe.OPTIONAL_HEADER.DllCharacteristics
        attr_dict["SizeOfHeaders"] = pe.OPTIONAL_HEADER.SizeOfHeaders
        attr_dict["Reserved1"] = pe.OPTIONAL_HEADER.Reserved1
        attr_dict["SizeOfHeapCommit"] = pe.OPTIONAL_HEADER.SizeOfHeapCommit
        attr_dict["MinorSubsystemVersion"] = pe.OPTIONAL_HEADER.MinorSubsystemVersion
        attr_dict["Subsystem"] = pe.OPTIONAL_HEADER.Subsystem
        attr_dict["MajorSubsystemVerison"] = pe.OPTIONAL_HEADER.MajorSubsystemVersion
        attr_dict["MajorOperatingSystemVersion"] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
        attr_dict["MinorOperatingSystemVersion"] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
        attr_dict["SizeOfImage"] = pe.OPTIONAL_HEADER.SizeOfImage
        attr_dict["ImageBase"] = pe.OPTIONAL_HEADER.ImageBase

    except Exception as e:
        print("")

import hashlib
import sys

def sha256_checksum(filename, path, block_size=65536):
    filename = str(path)+str('/'+filename)
    sha256 = hashlib.sha256()
    with open(filename, 'rb') as f:
        for block in iter(lambda: f.read(block_size), b''):
            sha256.update(block)
    return sha256.hexdigest()


# In[ ]:





# In[2]:


path = '/home/mohit/Desktop/Dynamic_binaries/Benign/0bc8af546901e6c20611c5250bd65acd0c4a8613bd8f8835f0d4680b5777f051'
path2 = '/home/mohit/Desktop/Dynamic_binaries/Benign'

# path = '/home/mohit/Desktop/Dynamic_binaries/Test_Both/0bb5d417c796065c871329ada335bcd056f741fa8ba8335a9f8ceedfe4e42669'
# path2 = '/home/mohit/Desktop/Dynamic_binaries/Test_Both'

#  getting the names of the files
pe = pefile.PE(path)
files = glob.glob(path2)
for name in files:
    list_of_files = (os.listdir(name))
    
##########################################################################################
# #                           Removing duplicates

temp_list_of_files = []
temp_list_of_files = list_of_files[:]
temp_hashes = []
for file in temp_list_of_files:
    h = sha256_checksum(file, path2)
    if h not in temp_hashes:
        temp_hashes.append(h)
#         temp_list_of_files.append(file)
    else:
        list_of_files.remove(file)

############################################################################################
    

# appending names to get the full path to files

# getting the list_of_dict_of_benign_files
for f in list_of_files:
    temp_dict = {}
    for file in glob.glob(os.path.join(path2, f)):
        try:
            pe = pefile.PE(file)
        except Exception as e:
            print("")
        DOS_HEADER()
        FILE_HEADER()
        OPTIONAL_HEADER()
#         print(attr_dict)
    attr_dict["Classifier"] = 0
    temp_dict = attr_dict.copy()
    list_of_benign_dict.append(temp_dict)
print('BENIGN: \n',list_of_benign_dict)



# In[ ]:





# In[3]:


#################################################################
#               Getting list of dict For Malicious File
# ===============================================================

path3 = '/home/mohit/Desktop/Dynamic_binaries/Malicious/0a143c2e6dabb31a31a6db8cb7a0f77558ae44a38687d5f831c87a98054f0ef9'
path4 = '/home/mohit/Desktop/Dynamic_binaries/Malicious'

# getting the names of the files
pe = pefile.PE(path3)
files = glob.glob(path4)
for name in files:
    list_of_files = (os.listdir(name))

##########################################################################################
# #                           Removing duplicates

temp_list_of_files = []
temp_list_of_files = list_of_files[:]
temp_hashes = []
for file in temp_list_of_files:
    h = sha256_checksum(file, path4)
    if h not in temp_hashes:
        temp_hashes.append(h)
#         temp_list_of_files.append(file)
    else:
        list_of_files.remove(file)

# list_of_files = temp_list_of_files[:]

############################################################################################
    

# appending names to get the full path to files

# getting the list_of_dict_of_benign_files
for f in list_of_files:
    temp_dict = {}
    for file in glob.glob(os.path.join(path4, f)):
        try:
            pe = pefile.PE(file)
        except Exception as e:
            print("")
        DOS_HEADER()
        FILE_HEADER()
        OPTIONAL_HEADER()
#         print(attr_dict)
    attr_dict["Classifier"] = 1
    temp_dict = attr_dict.copy()
    list_of_malicious_dict.append(temp_dict)
print('\n MALICIOUS: \n', list_of_malicious_dict)




# In[ ]:





# In[4]:


##########################################################################################
#                                       Plotting

df = pd.DataFrame(list_of_benign_dict)
df2 = pd.DataFrame(list_of_malicious_dict)

df = df.fillna(0) 
df2 = df2.fillna(0)

print(df)
print(df2)

# merging both dataframes
big_data = df.append(df2, ignore_index=True)

# getting the target column out of dataframe
target = big_data['Classifier']
big_data.pop('Classifier')
# print(target)
# print(big_data)

get_ipython().magic('matplotlib inline')

# getting the list of keys
list_of_keys = []
for dic in list_of_benign_dict:
    for key in dic.keys():
        list_of_keys.append(key)
list_of_keys = set(list_of_keys)
# print(list_of_keys)

# getting each feature from benign in a different list
to_plot = []
for key in list_of_keys:
    temp_list =[]
    for dic in list_of_benign_dict:
        if key in dic:
            new_dic = {}
            new_dic[key+str('_benign')] = dic[key]
            temp_list.append(new_dic)
    to_plot.append(temp_list)
print(to_plot)


# In[ ]:





# In[5]:


# getting each feature from malicious in the same dict
to_plot_malicious = []
for key in list_of_keys:
    temp_list = []
    for dic in list_of_malicious_dict:
        if key in dic:
            new_dic = {}
            new_dic[key+str('_malicious')] = dic[key]
            temp_list.append(new_dic)
    to_plot_malicious.append(temp_list)
print('\n \n', to_plot_malicious)



# In[ ]:





# In[6]:


# merging both list of list of dict
from itertools import cycle
final_list = []
for a, b in zip(to_plot, to_plot_malicious):
    temp_list = []
    for d1, d2 in zip(a, b):
        d1.update(d2)
        temp_list.append(d1)
    final_list.append(temp_list)
# print('\n \n \n \n \n \n \n \n', final_list)

################################################################
#                 Trying to Plot each feature
# ==============================================================
import matplotlib.pyplot as plt

for l in final_list:
    plt.figure()
    df4 = pd.DataFrame(l)
    print(df4)
    plt.title('HAHAHA it works!')
    df4.boxplot()
    plt.show()


# In[ ]:





# In[7]:


X = big_data  # is the merged dataframe of both bengin and malicious files
y = target  # is the classifier column in the big_data


# In[ ]:





# In[8]:



################################################################
#                       Training Model

# Benign
# path = 'C:/Users/momo/Desktop/Test_Benign/0ff6cdba44516cddf09411e767c599291cfe0ab9268be8e5f452845fad85226c'
# path2 = 'C:/Users/momo/Desktop/Test_Benign'
# #Malicious
# path = 'C:/Users/momo/Desktop/Test_Malicious/000b7774e51d0a788917b6966fae175ddfd5e2051021fbf89a2380bad14793fd'
# path2 = 'C:/Users/momo/Desktop/Test_Malicious'
# Test Both
# path = '/home/mohit/Desktop/Dynamic_binaries/check_benign/0bc8af546901e6c20611c5250bd65acd0c4a8613bd8f8835f0d4680b5777f051'
# path2 = '/home/mohit/Desktop/Dynamic_binaries/check_benign'
path = '/home/mohit/Desktop/Dynamic_binaries/Test_Both/0a0c98cc7a2dc1fc27a977e7cd507504f9999921a53d4f5b78f638738826c978'
path2 = '/home/mohit/Desktop/Dynamic_binaries/Test_Both'


pe = pefile.PE(path)
files = glob.glob(path2)
for name in files:
    list_of_files = (os.listdir(name))
    

list_train = []
# getting the files to test
for f in list_of_files:
    temp_dict = {}
    for file in glob.glob(os.path.join(path2, f)):
        try:
            pe = pefile.PE(file)
        except Exception as e:
            print("")
        DOS_HEADER()
        FILE_HEADER()
        OPTIONAL_HEADER()

    temp_dict = attr_dict.copy()
    list_train.append(temp_dict)

df_training = pd.DataFrame(list_train)
df_training.pop('Classifier')
X_new = df_training
# # print(X_new)




# In[ ]:





# In[9]:


#################################################Cross Validation and Parameter tuning
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score

knn = KNeighborsClassifier(n_neighbors = 10)
scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
print(scores)
print(scores.mean())

# Tunning parameters
k_range = range(1, 31)
k_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')
    (k_scores.append(scores.mean()))
print(k_scores)
knn.fit(X, y)
print(knn.predict(X_new))

# PLOTTING FOR BETTER VISUALS
get_ipython().magic('matplotlib inline')
#plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)
plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-validated Accuracy')



# In[ ]:





# In[10]:


################################################# GRID SEARCH ########################################################3
from sklearn.grid_search import GridSearchCV

k_range = list(range(1, 31))
print(k_range)

param_grid = dict(n_neighbors=k_range)
grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')

grid.fit(X, y)

# create a list of mean scores
grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]
print(grid_mean_scores)

#plot the results
plt.plot(k_range, grid_mean_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')

print(grid.best_score_)
print(grid.best_params_)
print(grid.best_estimator_)


# In[ ]:





# In[11]:


################################################  RANDOMIZED  SEARCH #########################################################

from sklearn.grid_search import RandomizedSearchCV

k_range = list(range(1, 20))
weight_options = ['uniform', 'distance']

param_dist = dict(n_neighbors = k_range, weights = weight_options)

print(param_dist)

rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)
rand.fit(X, y)
rand.grid_scores_

# examine the best model
print(rand.best_score_)
print(rand.best_params_)

# run RandomizedSearchCV 20 times (with n_iter = 10) and record the best score
best_scores = []
for _ in range(20):
    rand = RandomizedSearchCV(knn, param_dist, cv=10, scoring='accuracy', n_iter=10)
    rand.fit(X, y)
    best_scores.append(round(rand.best_score_, 3))
print(best_scores)


# In[ ]:





# In[12]:


# Decision Tree
from sklearn import tree
d_clf = tree.DecisionTreeClassifier()
d_clf = d_clf.fit(X, y)
print(d_clf.predict(X_new))
print(d_clf.score(X,y))


# In[ ]:





# In[13]:


# Random Forest 
from sklearn.ensemble import RandomForestRegressor

# # Create a random dataset
# X_train, X_test, y_train, y_test = train_test_split(X, y,
#                                                     train_size=400,
#                                                     random_state=4)
max_depth = 30

regr_rf = RandomForestRegressor()
regr_rf.fit(X, y)

# Predict on new data
y_rf = regr_rf.predict(X_new)
print(y_rf)
print(regr_rf.score(X,y))


# 

# In[14]:


#RandomForest Classifier
# Random Forest 
from sklearn.ensemble import RandomForestClassifier

# # Create a random dataset
# X_train, X_test, y_train, y_test = train_test_split(X, y,
#                                                     train_size=400,
#                                                     random_state=4)
max_depth = 30

regr_rf = RandomForestRegressor()
regr_rf.fit(X, y)

# Predict on new data
y_rf = regr_rf.predict(X_new)
print(y_rf)
print(regr_rf.score(X,y))

